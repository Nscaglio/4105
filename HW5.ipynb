{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8mJ2UpD3Cf14B8BQNQQkr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nscaglio/4105/blob/main/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 1**"
      ],
      "metadata": {
        "id": "3zbqFCA1cCWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bn2DC59b9j1",
        "outputId": "7003c96d-36fb-4b98-b748-a9b7c4ae16ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 4.844501\n",
            "Epoch 1000, Loss 3.785644\n",
            "Epoch 1500, Loss 3.466676\n",
            "Epoch 2000, Loss 3.406405\n",
            "Epoch 2500, Loss 3.377975\n",
            "Epoch 3000, Loss 3.344335\n",
            "Epoch 3500, Loss 3.301921\n",
            "Epoch 4000, Loss 3.249141\n",
            "Epoch 4500, Loss 3.184418\n",
            "Epoch 5000, Loss 3.106353\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)\n",
        "#1.a\n",
        "def model(t_u, w1, w2 , b):\n",
        "  return w2 * t_u ** 2 + w1 * t_u + b\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
        "  optimizer = optim.Adam([params], lr=learning_rate)\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch {epoch}, Loss {float(loss):.6f}')\n",
        "\n",
        "  return params\n",
        "params1 = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "params2 = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "params3 = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "params4 = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "#1.b\n",
        "# Train the model\n",
        "trained_params1 = training_loop(n_epochs=5000, learning_rate=0.1, params=params1, t_u=t_u, t_c=t_c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_params2= training_loop(n_epochs=5000, learning_rate=0.01, params=params2, t_u=t_u, t_c=t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oaiA3OOnBYt",
        "outputId": "00fda5d7-af9b-4b26-e284-f1d0963a6cbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 6.108693\n",
            "Epoch 1000, Loss 5.984358\n",
            "Epoch 1500, Loss 5.809032\n",
            "Epoch 2000, Loss 5.586317\n",
            "Epoch 2500, Loss 5.320902\n",
            "Epoch 3000, Loss 5.021267\n",
            "Epoch 3500, Loss 4.701309\n",
            "Epoch 4000, Loss 4.380754\n",
            "Epoch 4500, Loss 4.083409\n",
            "Epoch 5000, Loss 3.832656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_params3= training_loop(n_epochs=5000, learning_rate=0.001, params=params3, t_u=t_u, t_c=t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMnIGhUPnD0o",
        "outputId": "83c55c89-3c57-4fbb-86ce-614dffa55090"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 3668163.000000\n",
            "Epoch 1000, Loss 768786.437500\n",
            "Epoch 1500, Loss 90021.125000\n",
            "Epoch 2000, Loss 4791.339844\n",
            "Epoch 2500, Loss 98.014191\n",
            "Epoch 3000, Loss 6.609710\n",
            "Epoch 3500, Loss 6.117270\n",
            "Epoch 4000, Loss 6.096753\n",
            "Epoch 4500, Loss 6.071090\n",
            "Epoch 5000, Loss 6.038404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_params4= training_loop(n_epochs=5000, learning_rate=0.0001, params=params4, t_u=t_u, t_c=t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwXWHOA4nD4B",
        "outputId": "6603206e-a0df-40cd-9c01-6df30f93aa2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 10577728.000000\n",
            "Epoch 1000, Loss 9524402.000000\n",
            "Epoch 1500, Loss 8545122.000000\n",
            "Epoch 2000, Loss 7634292.500000\n",
            "Epoch 2500, Loss 6787368.000000\n",
            "Epoch 3000, Loss 6000706.000000\n",
            "Epoch 3500, Loss 5271407.500000\n",
            "Epoch 4000, Loss 4597170.000000\n",
            "Epoch 4500, Loss 3976134.250000\n",
            "Epoch 5000, Loss 3406753.750000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the learning rate of 0.1 did the best as it converged at a reasonable rate.\n"
      ],
      "metadata": {
        "id": "Ww9xCdZNn4x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2"
      ],
      "metadata": {
        "id": "9vkacQxZpSW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/ECGR4105/Datasets/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n",
        "varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "housing[varlist] = housing[varlist].apply(binary_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW8QLan7qX2B",
        "outputId": "f152dcc8-86b5-4e9c-ad2d-9fcb74887604"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X1 = housing[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']]\n",
        "y = housing['price']\n",
        "# Normalize the features (X1)\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler on the training data and transform both the training and validation data\n",
        "X1 = scaler.fit_transform(X1)  # Normalization\n",
        "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "\n",
        "\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "-hExUo4XtLYJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u, w, b):\n",
        "  return torch.matmul(t_u, w) + b  # Linear combination of features + bias\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "def training_loop(n_epochs, learning_rate, w, b, t_u_train, t_c_train, t_u_val, t_c_val):\n",
        "    # Set up Adam optimizer\n",
        "    optimizer = optim.Adam([w, b], lr=learning_rate)\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        # Forward pass on training data\n",
        "        t_p_train = model(t_u_train, w, b)\n",
        "        # Compute the training loss\n",
        "        train_loss = loss_fn(t_p_train, t_c_train)\n",
        "        # Backpropagation: compute gradients\n",
        "        train_loss.backward()\n",
        "        # Parameter update: adjust parameters using Adam\n",
        "        optimizer.step()\n",
        "        # Compute the validation loss\n",
        "        with torch.no_grad():  # No need to compute gradients during evaluation\n",
        "            t_p_val = model(t_u_val, w, b)\n",
        "            val_loss = loss_fn(t_p_val, t_c_val)\n",
        "        # Print loss every 500 epochs\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch {epoch}, Train Loss {float(train_loss):.6f}, Val Loss {float(val_loss):.6f}')\n",
        "    return w, b\n",
        "# Initialize parameters: weights (w) and bias (b)\n",
        "num_features = X_train.shape[1]  # Number of input features\n",
        "w1 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b1 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "\n",
        "# Training the model using the training data (X_train, y_train)\n",
        "n_epochs = 5000\n",
        "learning_rate = 0.1\n",
        "w1, b1 = training_loop(n_epochs, learning_rate, w1, b1, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nTxoxS-wx4B",
        "outputId": "a5ce84a1-44ba-4017-d075-78a416fcf443"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25233981440000.000000, Val Loss 30128813899776.000000\n",
            "Epoch 1000, Train Loss 25233169842176.000000, Val Loss 30127624814592.000000\n",
            "Epoch 1500, Train Loss 25232349855744.000000, Val Loss 30126435729408.000000\n",
            "Epoch 2000, Train Loss 25231531966464.000000, Val Loss 30125242449920.000000\n",
            "Epoch 2500, Train Loss 25230711980032.000000, Val Loss 30124051267584.000000\n",
            "Epoch 3000, Train Loss 25229894090752.000000, Val Loss 30122857988096.000000\n",
            "Epoch 3500, Train Loss 25229078298624.000000, Val Loss 30121668902912.000000\n",
            "Epoch 4000, Train Loss 25228260409344.000000, Val Loss 30120479817728.000000\n",
            "Epoch 4500, Train Loss 25227442520064.000000, Val Loss 30119290732544.000000\n",
            "Epoch 5000, Train Loss 25226626727936.000000, Val Loss 30118099550208.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "w2 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b2 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "w2, b2 = training_loop(n_epochs, learning_rate, w2, b2, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWozZ1W41HWV",
        "outputId": "4f759600-168a-4765-878e-e97c7cfc2fff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234713346048.000000, Val Loss 30129879252992.000000\n",
            "Epoch 1000, Train Loss 25234631557120.000000, Val Loss 30129759715328.000000\n",
            "Epoch 1500, Train Loss 25234547671040.000000, Val Loss 30129644371968.000000\n",
            "Epoch 2000, Train Loss 25234470076416.000000, Val Loss 30129520640000.000000\n",
            "Epoch 2500, Train Loss 25234388287488.000000, Val Loss 30129403199488.000000\n",
            "Epoch 3000, Train Loss 25234304401408.000000, Val Loss 30129281564672.000000\n",
            "Epoch 3500, Train Loss 25234222612480.000000, Val Loss 30129164124160.000000\n",
            "Epoch 4000, Train Loss 25234145017856.000000, Val Loss 30129050877952.000000\n",
            "Epoch 4500, Train Loss 25234061131776.000000, Val Loss 30128929243136.000000\n",
            "Epoch 5000, Train Loss 25233979342848.000000, Val Loss 30128809705472.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "w4 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b4 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "w4, b4 = training_loop(n_epochs, learning_rate, w4, b4, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9YmhEiU1hqw",
        "outputId": "24448768-a9da-4423-9be6-e700f0141140"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234788843520.000000, Val Loss 30129990402048.000000\n",
            "Epoch 1000, Train Loss 25234788843520.000000, Val Loss 30129990402048.000000\n",
            "Epoch 1500, Train Loss 25234786746368.000000, Val Loss 30129988304896.000000\n",
            "Epoch 2000, Train Loss 25234786746368.000000, Val Loss 30129988304896.000000\n",
            "Epoch 2500, Train Loss 25234786746368.000000, Val Loss 30129986207744.000000\n",
            "Epoch 3000, Train Loss 25234784649216.000000, Val Loss 30129986207744.000000\n",
            "Epoch 3500, Train Loss 25234784649216.000000, Val Loss 30129986207744.000000\n",
            "Epoch 4000, Train Loss 25234784649216.000000, Val Loss 30129984110592.000000\n",
            "Epoch 4500, Train Loss 25234784649216.000000, Val Loss 30129982013440.000000\n",
            "Epoch 5000, Train Loss 25234784649216.000000, Val Loss 30129982013440.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "w3 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b3 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "w3, b3 = training_loop(n_epochs, learning_rate, w3, b3, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGCNQ7791gFw",
        "outputId": "5d27249b-f7ae-4335-e0b8-901f06e75294"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234782552064.000000, Val Loss 30129984110592.000000\n",
            "Epoch 1000, Train Loss 25234774163456.000000, Val Loss 30129967333376.000000\n",
            "Epoch 1500, Train Loss 25234767872000.000000, Val Loss 30129958944768.000000\n",
            "Epoch 2000, Train Loss 25234755289088.000000, Val Loss 30129944264704.000000\n",
            "Epoch 2500, Train Loss 25234751094784.000000, Val Loss 30129933778944.000000\n",
            "Epoch 3000, Train Loss 25234740609024.000000, Val Loss 30129919098880.000000\n",
            "Epoch 3500, Train Loss 25234730123264.000000, Val Loss 30129906515968.000000\n",
            "Epoch 4000, Train Loss 25234725928960.000000, Val Loss 30129896030208.000000\n",
            "Epoch 4500, Train Loss 25234715443200.000000, Val Loss 30129887641600.000000\n",
            "Epoch 5000, Train Loss 25234709151744.000000, Val Loss 30129872961536.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the high learning rate is the best for the regression because it converges the quickest, it's just that the loss values are massive.\n",
        "generally it did better than homework 1 where the loss was just NaN"
      ],
      "metadata": {
        "id": "q-kdFB6A5b_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3**"
      ],
      "metadata": {
        "id": "DJo3aoUF55DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trinary_map(x):\n",
        "    mapping = {'furnished': 1, 'semi-furnished': 0.5, 'unfurnished': 0}\n",
        "    return mapping.get(x, 0)  # Default to 0 if the value is not in the mapping\n",
        "housing['furnishingstatus'] = housing['furnishingstatus'].apply(trinary_map)\n",
        "X2 = housing[['area',\t'bedrooms',\t'bathrooms',\t'stories',\t'mainroad',\t'guestroom',\t'basement',\t'hotwaterheating',\t'airconditioning',\t'parking',\t'prefarea','furnishingstatus']]\n",
        "y = housing['price']\n",
        "X2 = scaler.fit_transform(X2)  # Normalization\n",
        "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
        "# Convert back to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "BxMhSp1Y58Sn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_train.shape[1]  # Number of input features\n",
        "w1 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b1 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "\n",
        "# Training the model using the training data (X_train, y_train)\n",
        "learning_rate = 0.1\n",
        "w1, b1 = training_loop(n_epochs, learning_rate, w1, b1, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkeUi93h8gi0",
        "outputId": "5aeb8409-8cb0-45a3-99f5-4e29c22e6a69"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25233983537152.000000, Val Loss 30128811802624.000000\n",
            "Epoch 1000, Train Loss 25233163550720.000000, Val Loss 30127618523136.000000\n",
            "Epoch 1500, Train Loss 25232349855744.000000, Val Loss 30126425243648.000000\n",
            "Epoch 2000, Train Loss 25231531966464.000000, Val Loss 30125238255616.000000\n",
            "Epoch 2500, Train Loss 25230711980032.000000, Val Loss 30124049170432.000000\n",
            "Epoch 3000, Train Loss 25229894090752.000000, Val Loss 30122855890944.000000\n",
            "Epoch 3500, Train Loss 25229080395776.000000, Val Loss 30121662611456.000000\n",
            "Epoch 4000, Train Loss 25228260409344.000000, Val Loss 30120475623424.000000\n",
            "Epoch 4500, Train Loss 25227442520064.000000, Val Loss 30119280246784.000000\n",
            "Epoch 5000, Train Loss 25226626727936.000000, Val Loss 30118093258752.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b2 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "\n",
        "# Training the model using the training data (X_train, y_train)\n",
        "learning_rate = 0.01\n",
        "w2, b2 = training_loop(n_epochs, learning_rate, w2, b2, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtgWG1s48x1v",
        "outputId": "df98bb63-b54d-400c-df63-56ede4e690e8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234694471680.000000, Val Loss 30129854087168.000000\n",
            "Epoch 1000, Train Loss 25234610585600.000000, Val Loss 30129732452352.000000\n",
            "Epoch 1500, Train Loss 25234528796672.000000, Val Loss 30129610817536.000000\n",
            "Epoch 2000, Train Loss 25234449104896.000000, Val Loss 30129493377024.000000\n",
            "Epoch 2500, Train Loss 25234365218816.000000, Val Loss 30129375936512.000000\n",
            "Epoch 3000, Train Loss 25234285527040.000000, Val Loss 30129256398848.000000\n",
            "Epoch 3500, Train Loss 25234201640960.000000, Val Loss 30129136861184.000000\n",
            "Epoch 4000, Train Loss 25234119852032.000000, Val Loss 30129015226368.000000\n",
            "Epoch 4500, Train Loss 25234038063104.000000, Val Loss 30128897785856.000000\n",
            "Epoch 5000, Train Loss 25233954177024.000000, Val Loss 30128782442496.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w3 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b3 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "\n",
        "# Training the model using the training data (X_train, y_train)\n",
        "learning_rate = 0.001\n",
        "w3, b3 = training_loop(n_epochs, learning_rate, w3, b3, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgS1C47h8x44",
        "outputId": "847d66a8-afd0-4278-caa8-211592a951bc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234774163456.000000, Val Loss 30129971527680.000000\n",
            "Epoch 1000, Train Loss 25234765774848.000000, Val Loss 30129963139072.000000\n",
            "Epoch 1500, Train Loss 25234759483392.000000, Val Loss 30129952653312.000000\n",
            "Epoch 2000, Train Loss 25234751094784.000000, Val Loss 30129935876096.000000\n",
            "Epoch 2500, Train Loss 25234742706176.000000, Val Loss 30129927487488.000000\n",
            "Epoch 3000, Train Loss 25234736414720.000000, Val Loss 30129912807424.000000\n",
            "Epoch 3500, Train Loss 25234725928960.000000, Val Loss 30129902321664.000000\n",
            "Epoch 4000, Train Loss 25234721734656.000000, Val Loss 30129891835904.000000\n",
            "Epoch 4500, Train Loss 25234711248896.000000, Val Loss 30129875058688.000000\n",
            "Epoch 5000, Train Loss 25234700763136.000000, Val Loss 30129862475776.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w4 = torch.randn(num_features, requires_grad=True)  # Random initialization of weights\n",
        "b4 = torch.randn(1, requires_grad=True)  # Random initialization of bias\n",
        "\n",
        "# Training the model using the training data (X_train, y_train)\n",
        "learning_rate = 0.0001\n",
        "w4, b4 = training_loop(n_epochs, learning_rate, w4, b4, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G61Yw2h8x8F",
        "outputId": "994d12fa-5907-4a3c-e9ce-a932c9ad81ac"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Train Loss 25234780454912.000000, Val Loss 30129973624832.000000\n",
            "Epoch 1000, Train Loss 25234774163456.000000, Val Loss 30129971527680.000000\n",
            "Epoch 1500, Train Loss 25234774163456.000000, Val Loss 30129973624832.000000\n",
            "Epoch 2000, Train Loss 25234774163456.000000, Val Loss 30129969430528.000000\n",
            "Epoch 2500, Train Loss 25234774163456.000000, Val Loss 30129967333376.000000\n",
            "Epoch 3000, Train Loss 25234774163456.000000, Val Loss 30129967333376.000000\n",
            "Epoch 3500, Train Loss 25234772066304.000000, Val Loss 30129963139072.000000\n",
            "Epoch 4000, Train Loss 25234772066304.000000, Val Loss 30129963139072.000000\n",
            "Epoch 4500, Train Loss 25234772066304.000000, Val Loss 30129961041920.000000\n",
            "Epoch 5000, Train Loss 25234772066304.000000, Val Loss 30129961041920.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the high learning rate is the best for the regression because it converges the quickest, it's just that the loss values are massive. it did not do much any better or worse than with less features."
      ],
      "metadata": {
        "id": "qmTZ0Pbu9dRX"
      }
    }
  ]
}